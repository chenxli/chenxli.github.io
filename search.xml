<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>对比学习在推荐系统中的应用</title>
    <url>/2023/08/07/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<h3 id="引言">引言</h3>
<p>对比学习的主要思想是将相似的样本的表示相近，而不相似的远离。对比学习可以用到监督和无监督的场景下，并且目前在CV、NLP、推荐等领域中取得了较好的性能。本文将介绍和总结对比学习的基础以及其应用</p>
<span id="more"></span>
<h3 id="对比学习基础">对比学习基础</h3>
<ol type="1">
<li><p>损失函数</p>
<ul>
<li><p>NCE（noise-contrastive estimation）：是估计统计模型的参数的一种方法，主要通过学习数据分布和噪声分布之间的区别。原始的NCE只包含一个正负样本对。广义的NCE包含多个正样本或负样本。下式中，<span class="math inline">\(x\)</span>表示数据，<span class="math inline">\(y\)</span>表示噪声。</p>
<p><span class="math inline">\(J_T(\theta)=\frac{1}{2T}\sum\limits_t \ln[h(x_t;\theta)]+\ln[1-h(y_t;\theta)]\)</span></p>
<p>其中：</p>
<p><span class="math inline">\(h(\bold{u};\theta)=\frac{1}{1+exp[-G(\bold{u};\theta)]}\)</span></p>
<p><span class="math inline">\(G(\bold{u};\theta)=\ln p_m(\bold{u};\theta)-\ln p_n(\bold{u})\)</span></p></li>
<li><p>InfoNCE: 在CPC中提出，使用分类交叉熵损失在一组负样本中识别正样本。公式如下：</p>
<p><span class="math inline">\(L_N=-\mathop{\mathbb{E}}\limits_X[\log \frac{f_k(x_{t+k},c_t)}{\sum_{x_j\in X} f_k(x_j, c_t)}]\)</span></p></li>
<li><p>Triplet loss: 三元组损失，最初是由谷歌在FaceNet中提出。主要用于识别在不同角度和姿势下的人脸。下式中加号在右下角表示<span class="math inline">\(max(x,0)\)</span>。<span class="math inline">\(x_i^a\)</span>表示anchor，<span class="math inline">\(x_i^p\)</span>表示同一张人脸的不同照片，作为正样本，<span class="math inline">\(x_i^n\)</span>表示其他人脸，作为负样本。下式主要是用于拉近<span class="math inline">\(x_i^a\)</span>和<span class="math inline">\(x_i^p\)</span>的距离而拉远<span class="math inline">\(x_i^a\)</span>和<span class="math inline">\(x_i^n\)</span>的距离。</p>
<p><span class="math inline">\(\sum\limits_i^N [\|f(x_i^a)-f(x_i^p)\|_2^2-\|f(x_i^a)-f(x_i^n)\|_2^2+\alpha]_+\)</span></p></li>
<li><p>N-pair Loss： Multi-Class N-pair loss，是将Triplet loss泛化到与多个负样本进行对比。</p>
<p><span class="math inline">\(\mathcal{L}(\{x,x^+,\{x_i\}_{i=1}^{N-1}\};f)=\log(1+\sum\limits_{i=1}^{N-1} \exp(f^T f_i-f^T f^+))=-\log\frac{\exp{(f^Tf^+)}}{\exp{(f^T f^+)+\sum\limits_{i=1}^{L-1}}\exp(f^Tf_i)}\)</span></p></li>
</ul></li>
<li><p>衡量标准</p>
<p>对比学习算法具有两个关键属性alignment和uniformity，很多有效的对比学习算法正是较好地满足了这两种性质。</p>
<ul>
<li><p>alignment：衡量正例样本间的近似程度；</p></li>
<li><p>uniformity：衡量特征向量在超球体上的分布的均匀性</p></li>
</ul>
<p>两种性质的评价指标，优化这两个指标能使得下游任务表现更好。</p>
<p><span class="math inline">\(\mathcal{L}(f;\alpha)\triangleq \mathbb{E}_{(x,y)\sim p_{pos}}[\|f(x)-f(y)\|_2^{\alpha}]\)</span></p>
<p><span class="math inline">\(\mathcal{L}_{uniform}(f;t)\triangleq \log \mathbb{E}_{ {x,y}\mathop{\sim}\limits^{i.i.d.P}P_{data} }[e^{-t\|f(x)-f(y)\|^2_2}]\)</span></p></li>
<li><p>关键技术</p>
<ul>
<li><p>正负样本的构造</p>
<ul>
<li><p>数据增强：给定训练数据，需要进行数据增强来得到更多正样本。正确有效的数据增强技术对于学习好的表征至关重要。比如SimCLR的实验表明，图片的随机裁剪和颜色失真是最有效的两种方式。而对于句子来说，删除或替换可能会导致语义的改变。</p></li>
<li><p>负样本构造：一般对比学习中使用in-batch negatives，将一个batch内的不相关数据看作负样本。</p></li>
<li><p>多个模态：正样本对可以是两种模态的数据，比如图片和图片对应描述。</p></li>
</ul></li>
<li><p>大的batch size</p>
<ul>
<li>在训练期间使用大的batch size是许多对比学习方法成功的一个关键因素。当batch size足够大时，能够提供大量的负样本，使得模型学习更好表征来区别不同样本。</li>
</ul></li>
</ul></li>
</ol>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>搜广推</category>
      </categories>
      <tags>
        <tag>对比学习</tag>
        <tag>深度学习</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
</search>
