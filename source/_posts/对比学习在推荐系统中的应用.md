---
title: 对比学习在推荐系统中的应用
date: 2023-08-07 21:56:12
categories:
    - 搜广推
tags: 
    - 对比学习
    - 深度学习
    - 推荐系统
---
### 引言

对比学习的主要思想是将相似的样本的表示相近，而不相似的远离。对比学习可以用到监督和无监督的场景下，并且目前在CV、NLP、推荐等领域中取得了较好的性能。本文将介绍和总结对比学习的基础以及其应用

<!--more-->

### 对比学习基础

1. 损失函数

   - NCE（noise-contrastive estimation）：是估计统计模型的参数的一种方法，主要通过学习数据分布和噪声分布之间的区别。原始的NCE只包含一个正负样本对。广义的NCE包含多个正样本或负样本。下式中，$x$表示数据，$y$表示噪声。
     ![img](https://raw.githubusercontent.com/chenxli/ImgStg/main/20230811000018.png)

     $J_T(\theta)=\frac{1}{2T}\sum\limits_t \ln[h(x_t;\theta)]+\ln[1-h(y_t;\theta)]$

     其中：

     $h(\bold{u};\theta)=\frac{1}{1+exp[-G(\bold{u};\theta)]}$

     $G(\bold{u};\theta)=\ln p_m(\bold{u};\theta)-\ln p_n(\bold{u})$

   - InfoNCE: 在CPC中提出，使用分类交叉熵损失在一组负样本中识别正样本。公式如下：

     $L_N=-\mathop{\mathbb{E}}\limits_X[\log \frac{f_k(x_{t+k},c_t)}{\sum_{x_j\in X} f_k(x_j, c_t)}]$

   - Triplet loss: 三元组损失，最初是由谷歌在FaceNet中提出。主要用于识别在不同角度和姿势下的人脸。下式中加号在右下角表示$max(x,0)$。$x_i^a$表示anchor，$x_i^p$表示同一张人脸的不同照片，作为正样本，$x_i^n$表示其他人脸，作为负样本。下式主要是用于拉近$x_i^a$和$x_i^p$的距离而拉远$x_i^a$和$x_i^n$的距离。

     $\sum\limits_i^N [\|f(x_i^a)-f(x_i^p)\|_2^2-\|f(x_i^a)-f(x_i^n)\|_2^2+\alpha]_+$

   - N-pair Loss： Multi-Class N-pair loss，是将Triplet loss泛化到与多个负样本进行对比。

     $\mathcal{L}(\{x,x^+,\{x_i\}_{i=1}^{N-1}\};f)=\log(1+\sum\limits_{i=1}^{N-1} \exp(f^T f_i-f^T f^+))=-\log\frac{\exp{(f^Tf^+)}}{\exp{(f^T f^+)+\sum\limits_{i=1}^{L-1}}\exp(f^Tf_i)}$

2. 衡量标准

   对比学习算法具有两个关键属性alignment和uniformity，很多有效的对比学习算法正是较好地满足了这两种性质。

   - alignment：衡量正例样本间的近似程度；

   - uniformity：衡量特征向量在超球体上的分布的均匀性

   两种性质的评价指标，优化这两个指标能使得下游任务表现更好。

   $$\mathcal{L}(f;\alpha)\triangleq \mathbb{E}_{(x,y)\sim p_{pos}}[\|f(x)-f(y)\|_2^{\alpha}]$$

   $$\mathcal{L}_{uniform}(f;t)\triangleq \log \mathbb{E}_{ {x,y}\mathop{\sim}\limits^{i.i.d.P}P_{data} }[e^{-t\|f(x)-f(y)\|^2_2}]$$

3. 关键技术

   - 正负样本的构造

     - 数据增强：给定训练数据，需要进行数据增强来得到更多正样本。正确有效的数据增强技术对于学习好的表征至关重要。比如SimCLR的实验表明，图片的随机裁剪和颜色失真是最有效的两种方式。而对于句子来说，删除或替换可能会导致语义的改变。

     - 负样本构造：一般对比学习中使用in-batch negatives，将一个batch内的不相关数据看作负样本。

     - 多个模态：正样本对可以是两种模态的数据，比如图片和图片对应描述。

   - 大的batch size

     - 在训练期间使用大的batch size是许多对比学习方法成功的一个关键因素。当batch size足够大时，能够提供大量的负样本，使得模型学习更好表征来区别不同样本。 

```C++
#include <iostream>
```